{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cRSlzm2Ev4oWlkOLOy729k_ihXlE5gvR",
      "authorship_tag": "ABX9TyOZ3syf8O1uaw5bczJLR2ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FizaKonnur06/ResumeAnalyzerProject0.2/blob/main/AI_Resume_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3RO6K6qRXvt",
        "outputId": "d4a9322f-e76a-448d-eea8-82253b9d950c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn PyPDF2 python-docx pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io # Added import for BytesIO\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import PyPDF2\n",
        "import docx"
      ],
      "metadata": {
        "id": "penpjWHJR4yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxOgtiNMR9YY",
        "outputId": "f3aa7dc9-01ce-4d2d-b01f-b4f4788ca182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MJL-2KxDUhLk",
        "outputId": "f40fd074-7d1e-403f-83b7-fb57477bc393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-481eb887-2850-472c-abf8-a007fcff91e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-481eb887-2850-472c-abf8-a007fcff91e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Fiza A Konnur.pdf to Fiza A Konnur.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(file_name, file_content):\n",
        "    text = \"\"\n",
        "\n",
        "    # Determine file type based on file_name extension\n",
        "    if file_name.lower().endswith(\".pdf\"):\n",
        "        reader = PyPDF2.PdfReader(io.BytesIO(file_content)) # Use BytesIO for content\n",
        "        for page in reader.pages:\n",
        "            extracted_page_text = page.extract_text()\n",
        "            if extracted_page_text: # Ensure text was actually extracted\n",
        "                text += extracted_page_text + \" \" # Add space for readability\n",
        "            else:\n",
        "                # Handle cases where extract_text might return None or empty string\n",
        "                # e.g., scanned PDFs without selectable text\n",
        "                print(f\"Warning: No text extracted from a page in {file_name}. It might be an image-based PDF.\")\n",
        "\n",
        "    elif file_name.lower().endswith(\".docx\"):\n",
        "        doc = docx.Document(io.BytesIO(file_content)) # Use BytesIO for content\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \" \"\n",
        "\n",
        "    return text\n",
        "\n",
        "resume_file_name = list(uploaded.keys())[0]\n",
        "resume_file_content = uploaded[resume_file_name] # Get the byte content\n",
        "resume_text = extract_text(resume_file_name, resume_file_content) # Pass both\n",
        "\n",
        "print(resume_text[:1000])   # preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCDUYA4GVBSh",
        "outputId": "1e6fad53-e848-49d1-d381-ff4c0cc2da8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIZA ABIDALI KONNUR\n",
            "CAREER OBJECTIVE\n",
            "Detail-oriented and motivated IT graduate seeking an entry-level IT Support / Service\n",
            "Desk role where I can assist end users with application, system, and network-related\n",
            "issues, utilize my technical and communication skills, and contribute effectively to\n",
            "organizational success while continuously learning and growing in the IT domain.8073998152\n",
            "EDUCATION\n",
            "SOFT SKILLShttps://github.com/FizaKonnur06\n",
            "Bachelor of Computer Application(BCA)-Rani Channamma University ,Belguam|2023-\n",
            "Present|CGPA:8.9\n",
            "PUC(Commerce)-Bapuji Gramin Vikas Samithi College, Ramnagar|2021-\n",
            "2023|Percentage:83.66%\n",
            "SSLC-Bashiban Urdu High School,Khanapur|2018-2021|Percentage:88.64%\n",
            "TECHNICAL SKILLS\n",
            " Programming Launguages:C,Python,java,C#(Basics) \n",
            "Web Technologies: HTML,CSS,Javascript(Basics)\n",
            "Database Management:MYSql  \n",
            "Tools and Platforms:Github,visual Studio,PowerBI\n",
            "Other Concepts: Data structure using C,OOPs,Operating System,Computer\n",
            "Communication Networks,Software engineering.\n",
            "Excel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bccdf99"
      },
      "source": [
        "### Job Description\n",
        "\n",
        "Here, we define the job description against which resumes will be compared. This description can be updated to match different job roles and requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48414e8c",
        "outputId": "3b2dc44d-9f43-410b-a4e9-c336cace9bc0"
      },
      "source": [
        "job_description = \"\"\"\n",
        "We are looking for a Python developer with knowledge of\n",
        "Machine Learning, NLP, SQL, Flask, and AWS.\n",
        "Experience in data analysis is a plus.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Job Description:\\n\", job_description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job Description:\n",
            " \n",
            "We are looking for a Python developer with knowledge of\n",
            "Machine Learning, NLP, SQL, Flask, and AWS.\n",
            "Experience in data analysis is a plus.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "clean_resume = clean_text(resume_text)\n",
        "clean_job = clean_text(job_description)\n"
      ],
      "metadata": {
        "id": "Ukt-gK2FV5NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skill_list = [\n",
        "    \"python\", \"java\", \"machine learning\", \"deep learning\",\n",
        "    \"sql\", \"nlp\", \"aws\", \"docker\", \"flask\",\n",
        "    \"data analysis\", \"pandas\", \"numpy\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "tYbuqEWHWUGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills(text, skills):\n",
        "    extracted = []\n",
        "    for skill in skills:\n",
        "        if skill in text:\n",
        "            extracted.append(skill)\n",
        "    return list(set(extracted))\n",
        "\n",
        "resume_skills = extract_skills(clean_resume, skill_list)\n",
        "job_skills = extract_skills(clean_job, skill_list)\n",
        "\n",
        "print(\"Resume Skills:\", resume_skills)\n",
        "print(\"Job Required Skills:\", job_skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X9FMK76WYZ8",
        "outputId": "4ba661f7-dc7f-4045-9ab1-02df72a5c4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume Skills: ['pandas', 'machine learning', 'python', 'sql', 'java', 'data analysis', 'numpy']\n",
            "Job Required Skills: ['nlp', 'machine learning', 'aws', 'python', 'sql', 'data analysis', 'flask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([clean_resume, clean_job])\n",
        "\n",
        "similarity_score = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
        "match_percentage = round(similarity_score * 100, 2)\n",
        "\n",
        "print(\"Match Percentage:\", match_percentage, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JExR7HISWcnq",
        "outputId": "0af41537-44d7-446e-9a85-b7ce94af7157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match Percentage: 10.11 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matched_skills = list(set(resume_skills) & set(job_skills))\n",
        "missing_skills = list(set(job_skills) - set(resume_skills))\n",
        "\n",
        "print(\"Matched Skills:\", matched_skills)\n",
        "print(\"Missing Skills:\", missing_skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pfb8zVQWgjD",
        "outputId": "504e9a32-04a0-40a3-ce12-85ecf0b26ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched Skills: ['machine learning', 'sql', 'data analysis', 'python']\n",
            "Missing Skills: ['nlp', 'flask', 'aws']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_job_skills = len(job_skills)\n",
        "matched_count = len(matched_skills)\n",
        "\n",
        "match_percentage = (matched_count / total_job_skills) * 100\n",
        "\n",
        "print(\"Match Percentage:\", round(match_percentage,2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5-n02n34-Ew",
        "outputId": "9571afd5-28ea-40f8-be19-1c7679bb94f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match Percentage: 57.14 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_count = len(missing_skills)\n",
        "skill_gap = (missing_count / total_job_skills) * 100\n",
        "\n",
        "print(\"Skill Gap:\", round(skill_gap,2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgUuHoov4_xW",
        "outputId": "fb138950-e880-47c3-ffcc-91cb73b15fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skill Gap: 42.86 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if match_percentage >= 75:\n",
        "    level = \"ðŸ”¥ Highly Suitable\"\n",
        "elif match_percentage >= 50:\n",
        "    level = \"ðŸ‘ Moderately Suitable\"\n",
        "else:\n",
        "    level = \"âš ï¸ Low Match â€“ Needs Improvement\"\n",
        "\n",
        "print(\"Candidate Level:\", level)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oU31xpr5EH6",
        "outputId": "cc73de0d-426e-4bbd-a232-563e131cbe87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate Level: ðŸ‘ Moderately Suitable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----- AI Career Recommendation -----\")\n",
        "\n",
        "if match_percentage >= 75:\n",
        "    print(\"Excellent! Your resume strongly matches this job role.\")\n",
        "    print(\"You can confidently apply for this position.\")\n",
        "\n",
        "elif match_percentage >= 50:\n",
        "    print(\"Good match! But improving the missing skills will increase your chances.\")\n",
        "    print(\"Focus on learning:\", \", \".join(missing_skills))\n",
        "\n",
        "else:\n",
        "    print(\"Your resume does not match well with this job.\")\n",
        "    print(\"You should learn these skills before applying:\")\n",
        "    print(\", \".join(missing_skills))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f21omBEw5H_F",
        "outputId": "632de376-8fcd-489a-c7f5-585c9a422cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- AI Career Recommendation -----\n",
            "Good match! But improving the missing skills will increase your chances.\n",
            "Focus on learning: nlp, flask, aws\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----- RESUME ANALYSIS -----\")\n",
        "\n",
        "total_resume_skills = len(resume_skills)\n",
        "print(\"Total Skills Found:\", total_resume_skills)\n",
        "print(\"Skills Detected:\", resume_skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-6C5tHKDL8t",
        "outputId": "a0c2652e-8897-4c91-c5c2-b21cffa1bc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- RESUME ANALYSIS -----\n",
            "Total Skills Found: 7\n",
            "Skills Detected: ['pandas', 'machine learning', 'python', 'sql', 'java', 'data analysis', 'numpy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_roles = [\"python\",\"machine learning\",\"data analysis\",\"statistics\",\"pandas\",\"numpy\",\"sql\",\"nlp\",\"deep learning\"]\n",
        "web_roles = [\"html\",\"css\",\"javascript\",\"react\",\"node\",\"mongodb\",\"express\"]\n",
        "cloud_roles = [\"aws\",\"azure\",\"gcp\",\"docker\",\"kubernetes\"]\n",
        "\n",
        "domain = \"General\"\n",
        "\n",
        "if any(skill in resume_skills for skill in data_roles):\n",
        "    domain = \"Data Science / Machine Learning\"\n",
        "\n",
        "elif any(skill in resume_skills for skill in web_roles):\n",
        "    domain = \"Web Development\"\n",
        "\n",
        "elif any(skill in resume_skills for skill in cloud_roles):\n",
        "    domain = \"Cloud / DevOps\"\n",
        "\n",
        "print(\"Detected Resume Domain:\", domain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjeM0Iz7DOdW",
        "outputId": "58bf7fa0-c8ea-484d-a1d4-e05a648f41fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Resume Domain: Data Science / Machine Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_roles = [\"python\",\"machine learning\",\"data analysis\",\"statistics\",\"pandas\",\"numpy\",\"sql\",\"nlp\",\"deep learning\"]\n",
        "web_roles = [\"html\",\"css\",\"javascript\",\"react\",\"node\",\"mongodb\",\"express\"]\n",
        "cloud_roles = [\"aws\",\"azure\",\"gcp\",\"docker\",\"kubernetes\"]\n",
        "\n",
        "domain = \"General\"\n",
        "\n",
        "if any(skill in resume_skills for skill in data_roles):\n",
        "    domain = \"Data Science / Machine Learning\"\n",
        "\n",
        "elif any(skill in resume_skills for skill in web_roles):\n",
        "    domain = \"Web Development\"\n",
        "\n",
        "elif any(skill in resume_skills for skill in cloud_roles):\n",
        "    domain = \"Cloud / DevOps\"\n",
        "\n",
        "print(\"Detected Resume Domain:\", domain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpPsntvfDVFX",
        "outputId": "178d6428-f64c-44c5-f02a-b7b11e588eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Resume Domain: Data Science / Machine Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if total_resume_skills >= 10:\n",
        "    experience = \"Experienced / Job Ready\"\n",
        "elif total_resume_skills >= 6:\n",
        "    experience = \"Intermediate\"\n",
        "else:\n",
        "    experience = \"Beginner / Fresher\"\n",
        "\n",
        "print(\"Experience Level:\", experience)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WkB3v4DDkab",
        "outputId": "da54d8f4-2458-44bd-8618-04be3be7e69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experience Level: Intermediate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_strength = min(total_resume_skills * 10, 100)\n",
        "print(\"Resume Strength Score:\", resume_strength, \"/100\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRhrgL1TDqWr",
        "outputId": "fbd37ef0-fff0-4929-ac89-3fd340850d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume Strength Score: 70 /100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----- Resume Summary -----\")\n",
        "\n",
        "summary = f\"\"\"\n",
        "This resume belongs to a {experience} candidate.\n",
        "The profile is mainly focused on {domain}.\n",
        "The resume shows {resume_strength}% strength based on skills detected.\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1VEWAoADyir",
        "outputId": "c2049740-e4bc-4fbb-c8eb-22263eed3ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Resume Summary -----\n",
            "\n",
            "This resume belongs to a Intermediate candidate.\n",
            "The profile is mainly focused on Data Science / Machine Learning.\n",
            "The resume shows 70% strength based on skills detected.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“Š AI Resume Analysis Report\")\n",
        "print(\"---------------------------\")\n",
        "print(f\"âœ” Match Score: {match_percentage}%\")\n",
        "print(f\"ðŸŸ¢ Matched Skills: {matched_skills}\")\n",
        "print(f\"ðŸ”´ Missing Skills: {missing_skills}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHKrL7q1Wlrb",
        "outputId": "3d9bbb6d-be6e-4487-dba1-e260187d821a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š AI Resume Analysis Report\n",
            "---------------------------\n",
            "âœ” Match Score: 57.14285714285714%\n",
            "ðŸŸ¢ Matched Skills: ['machine learning', 'sql', 'data analysis', 'python']\n",
            "ðŸ”´ Missing Skills: ['nlp', 'flask', 'aws']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok nltk scikit-learn PyPDF2 python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Jt3xYXgUer",
        "outputId": "554d9750-e242-45d1-ac6a-8a61403a19a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.53.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.6)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import PyPDF2\n",
        "import docx\n",
        "\n",
        "# Download NLTK data (safe in Streamlit)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ---------- FUNCTIONS ----------\n",
        "\n",
        "def extract_text(uploaded_file):\n",
        "    text = \"\"\n",
        "    if uploaded_file.name.endswith(\".pdf\"):\n",
        "        reader = PyPDF2.PdfReader(uploaded_file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    elif uploaded_file.name.endswith(\".docx\"):\n",
        "        doc = docx.Document(uploaded_file)\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \" \"\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def extract_skills(text):\n",
        "    skills = [\n",
        "        \"python\",\"java\",\"machine learning\",\"deep learning\",\n",
        "        \"sql\",\"aws\",\"flask\",\"pandas\"\n",
        "    ]\n",
        "    return [s for s in skills if s in text]\n",
        "\n",
        "# ---------- STREAMLIT UI ----------\n",
        "\n",
        "st.title(\"AI Resume Analyzer & Job Matcher\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload Resume (PDF or DOCX)\", type=[\"pdf\",\"docx\"])\n",
        "job_desc = st.text_area(\"Paste Job Description\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if uploaded_file and job_desc:\n",
        "        resume_text = extract_text(uploaded_file)\n",
        "        clean_resume = clean_text(resume_text)\n",
        "        clean_job = clean_text(job_desc)\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectors = vectorizer.fit_transform([clean_resume, clean_job])\n",
        "        score = cosine_similarity(vectors[0:1], vectors[1:2])[0][0] * 100\n",
        "\n",
        "        resume_skills = extract_skills(clean_resume)\n",
        "        job_skills = extract_skills(clean_job)\n",
        "\n",
        "        matched = list(set(resume_skills) & set(job_skills))\n",
        "        missing = list(set(job_skills) - set(resume_skills))\n",
        "\n",
        "        st.success(f\"Match Score: {round(score,2)}%\")\n",
        "        st.write(\"Matched Skills:\", matched)\n",
        "        st.write(\"Missing Skills:\", missing)\n",
        "    else:\n",
        "        st.warning(\"Please upload a resume and paste job description.\")\n"
      ],
      "metadata": {
        "id": "ebyRAOybhfZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "pMiN5Y2RhoQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 39CBEJaCBoRY7BHQ0XwrvtxMjEE_3XbHfbMomkkEwQSFdJd1t\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6oS4uXsl_qJ",
        "outputId": "96c48e2e-629c-4b79-9288-f7bf9a450adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your app is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLh6RWICmXd-",
        "outputId": "d8e320fd-de12-4d23-ba6a-f870d7395bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your app is live at: NgrokTunnel: \"https://personable-margo-euphonically.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942c2701"
      },
      "source": [
        "# Task\n",
        "Analyze the resume `FIZA ABIDALI KONNUR.pdf` against the provided job description to identify relevant skills, calculate a match percentage, and generate a report summarizing matched and missing skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8df2194"
      },
      "source": [
        "## Project Overview\n",
        "\n",
        "### Subtask:\n",
        "Provide an introductory note explaining the purpose of this project, which is to analyze and match a resume against a given job description.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdeb8827"
      },
      "source": [
        "## Project Overview\n",
        "\n",
        "### Subtask:\n",
        "Provide an introductory note explaining the purpose of this project, which is to analyze and match a resume against a given job description.\n",
        "\n",
        "### Introduction\n",
        "This project aims to analyze a provided resume against a specific job description. It identifies relevant skills present in the resume, compares them with the skills required by the job description, and calculates a match percentage to indicate the resume's suitability for the role. This tool can help job seekers tailor their resumes and recruiters efficiently screen candidates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d365ff9"
      },
      "source": [
        "## File Upload and Text Extraction\n",
        "\n",
        "### Subtask:\n",
        "Detail how the resume file was uploaded and how text was extracted from it using PyPDF2, ensuring it can handle various document formats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02638505"
      },
      "source": [
        "### File Upload and Text Extraction\n",
        "\n",
        "To begin, the resume file was uploaded to the Colab environment using `google.colab.files.upload()`. This function provides a convenient way for users to upload files directly from their local system into the Colab runtime, making them accessible for processing.\n",
        "\n",
        "Following the upload, a custom function named `extract_text` was implemented to handle text extraction from various document formats. This function is designed to intelligently parse both PDF and DOCX files:\n",
        "\n",
        "- **For PDF files**: It utilizes the `PyPDF2.PdfReader` library. The uploaded file content, which is in bytes, is first wrapped with `io.BytesIO` to allow `PyPDF2` to read it as a file-like object. The function then iterates through each page of the PDF, using `page.extract_text()` to retrieve the textual content. A warning is printed if a page yields no text, which can happen with image-based PDFs.\n",
        "\n",
        "- **For DOCX files**: It uses the `docx.Document` library. Similar to PDF handling, `io.BytesIO` is employed to process the byte content of the DOCX file. The function then iterates through all paragraphs in the document, concatenating their text content.\n",
        "\n",
        "The `io.BytesIO` object plays a crucial role by providing an in-memory binary stream, making the uploaded file content compatible with both `PyPDF2` and `docx` libraries, which expect file-like objects as input.\n",
        "\n",
        "Finally, the extracted text from the uploaded resume is stored in the `resume_text` variable. A preview of the first 1000 characters of `resume_text` was then printed to verify that the text extraction was successful and to get a glimpse of the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ec4bd14"
      },
      "source": [
        "## Job Description Definition\n",
        "\n",
        "### Subtask:\n",
        "Explain how the job description was defined and why it's a crucial input for the matching process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2b4de64"
      },
      "source": [
        "### Job Description Definition\n",
        "\n",
        "#### How the `job_description` was defined:\n",
        "\n",
        "The `job_description` variable was explicitly defined in the notebook as a multi-line string. This allows for a detailed and human-readable representation of the desired job role and its requirements. For instance, in the previous code, it was set as:\n",
        "\n",
        "```python\n",
        "job_description = \"\"\"\n",
        "We are looking for a Python developer with knowledge of\n",
        "Machine Learning, NLP, SQL, Flask, and AWS.\n",
        "Experience in data analysis is a plus.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### Why it's a crucial input for the matching process:\n",
        "\n",
        "The `job_description` is a critical input because it serves as the **benchmark** or **reference point** against which each resume's suitability is evaluated. The entire matching process, including skill extraction and similarity calculation, hinges on understanding what the job requires. Without a clear and comprehensive job description, the system would not know what skills to look for or how to assess the relevance of the resume content. It directly dictates the 'ideal candidate profile', allowing the algorithm to quantify how well a resume aligns with those specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "753443ef"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Describe the text cleaning and normalization steps applied to both the resume and job description, including lowercasing, punctuation removal, tokenization, stop words removal, and lemmatization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37d68f08"
      },
      "source": [
        "### Text Preprocessing Description\n",
        "\n",
        "Text preprocessing is a crucial step in preparing raw text data for analysis. Its purpose is to transform unstructured text into a clean and standardized format, which helps in improving the accuracy and efficiency of subsequent analytical tasks, such as skill extraction and similarity calculation. By cleaning and normalizing the text, we reduce noise, handle variations in word forms, and focus on the most meaningful terms.\n",
        "\n",
        "Our `clean_text` function performs the following steps:\n",
        "\n",
        "1.  **Lowercasing**: All text is converted to lowercase (`text.lower()`). This ensures that words like \"Python\", \"python\", and \"PYTHON\" are treated as the same word, preventing the system from considering them distinct entities.\n",
        "\n",
        "2.  **Punctuation Removal**: Punctuation marks (e.g., periods, commas, exclamation marks) are removed using `text.translate(str.maketrans(\"\", \"\", string.punctuation))`. Punctuation typically doesn't contribute to the meaning of keywords or skills and can introduce unnecessary complexity.\n",
        "\n",
        "3.  **Tokenization**: The cleaned text is broken down into individual words or \"tokens\" using `nltk.word_tokenize(text)`. This creates a list of words that can be processed independently.\n",
        "\n",
        "4.  **Stop Words Removal**: Common English stop words (e.g., 'the', 'is', 'and', 'a') are filtered out (`[w for w in tokens if w not in stopwords.words('english')]`). These words are generally high-frequency but carry little semantic value for identifying specific skills or topics, so removing them helps to focus on more significant terms.\n",
        "\n",
        "5.  **Lemmatization**: A `WordNetLemmatizer` is applied to reduce words to their base or dictionary form (`[lemmatizer.lemmatize(w) for w in tokens]`). For example, \"running,\" \"ran,\" and \"runs\" are all reduced to \"run.\" This helps in consolidating different inflections of a word into a single canonical form, improving the consistency of our analysis.\n",
        "\n",
        "This comprehensive `clean_text` function was applied to both the original `resume_text` to generate `clean_resume` and to the `job_description` to produce `clean_job`, ensuring a standardized and clean input for further processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9c9d9e4"
      },
      "source": [
        "## Skill Extraction\n",
        "\n",
        "### Subtask:\n",
        "Outline the process of identifying and extracting relevant skills from both the preprocessed resume and job description based on a predefined skill list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f2470e1"
      },
      "source": [
        "## Skill Extraction\n",
        "\n",
        "### Subtask:\n",
        "Outline the process of identifying and extracting relevant skills from both the preprocessed resume and job description based on a predefined skill list.\n",
        "\n",
        "#### Explanation of Skill Extraction Process:\n",
        "\n",
        "1.  **Predefined Skill List**: A `skill_list` (e.g., `\"python\", \"sql\", \"machine learning\"`) serves as a comprehensive reference of relevant technical skills. This list is manually curated to include skills pertinent to the roles being matched.\n",
        "\n",
        "2.  **`extract_skills` Function**: The core of the skill extraction process is the `extract_skills` function. This function takes two main arguments:\n",
        "    *   `text`: The preprocessed and cleaned text (either from the resume or the job description).\n",
        "    *   `skills`: The `skill_list` defined above.\n",
        "\n",
        "3.  **Iteration and Identification**: Inside the `extract_skills` function, it iterates through each skill in the `skill_list`. For every skill, it checks if that skill string is present within the input `text`. This check is case-insensitive due to the prior cleaning step where all text was converted to lowercase.\n",
        "\n",
        "4.  **Unique Skill Compilation**: If a skill from the `skill_list` is found in the input `text`, it is added to a temporary list. To avoid duplicate entries, the function ensures that only unique skills are returned by converting the list to a set and then back to a list.\n",
        "\n",
        "5.  **Application to Resume and Job Description**: This `extract_skills` function is then applied separately to both the `clean_resume` and `clean_job` texts:\n",
        "    *   `resume_skills = extract_skills(clean_resume, skill_list)`: This call identifies all skills from the predefined `skill_list` that are present in the candidate's resume.\n",
        "    *   `job_skills = extract_skills(clean_job, skill_list)`: This call identifies all skills from the predefined `skill_list` that are mentioned in the job description.\n",
        "\n",
        "6.  **Verification**: Finally, printing the `resume_skills` and `job_skills` lists allows for immediate verification of the extracted skills from both sources. This step confirms that the process correctly identified the relevant skills based on the `skill_list` and provides a clear overview of what each document contains in terms of technical competencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6996cda7"
      },
      "source": [
        "## Similarity Calculation\n",
        "\n",
        "### Subtask:\n",
        "Explain the methodology used to calculate the similarity score between the resume and job description using TF-IDF vectorization and cosine similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "356be034"
      },
      "source": [
        "```markdown\n",
        "## Methodology for Similarity Calculation\n",
        "\n",
        "To determine the similarity between the resume and the job description, a two-step process involving TF-IDF vectorization and cosine similarity is employed.\n",
        "\n",
        "### 1. TF-IDF Vectorization\n",
        "\n",
        "**Purpose:** TF-IDF (Term Frequency-Inverse Document Frequency) vectorization converts text documents into numerical representations, which are essential for computational analysis. It weighs words based on their frequency in a document and their rarity across all documents, giving more importance to words that are unique and relevant to a specific document.\n",
        "\n",
        "**Implementation:**\n",
        "- The `TfidfVectorizer` from `sklearn.feature_extraction.text` is initialized. This object is responsible for transforming text into a matrix of TF-IDF features.\n",
        "- The `fit_transform` method of the `TfidfVectorizer` is then applied to a list containing both the `clean_resume` and `clean_job` texts. This step performs two main functions:\n",
        "    - **`fit`**: It learns the vocabulary and inverse document frequency (IDF) weights from the provided texts.\n",
        "    - **`transform`**: It converts the texts into numerical vectors. Each document (resume and job description) becomes a vector, where each dimension corresponds to a word in the learned vocabulary, and its value represents the TF-IDF score of that word in the document. A higher score indicates greater importance of that word within the document relative to the entire corpus (in this case, the resume and job description together).\n",
        "\n",
        "### 2. Cosine Similarity Calculation\n",
        "\n",
        "**Purpose:** Cosine similarity is a metric used to measure how similar two non-zero vectors are. It measures the cosine of the angle between two vectors. A cosine similarity of 1 means the vectors are identical (same direction), 0 means they are orthogonal (no similarity), and -1 means they are opposite.\n",
        "\n",
        "**Implementation:**\n",
        "- The `cosine_similarity` function from `sklearn.metrics.pairwise` is used to calculate the similarity between the two vectorized documents.\n",
        "- Specifically, it compares the vector of the cleaned resume (`vectors[0:1]`) with the vector of the cleaned job description (`vectors[1:2]`). The output is a matrix, and we extract the single similarity score from it.\n",
        "\n",
        "**Match Percentage:**\n",
        "- The resulting `similarity_score` (a value between 0 and 1) is then multiplied by 100 to convert it into a percentage.\n",
        "- This percentage is rounded to two decimal places to provide a clear and readable `match_percentage`.\n",
        "- Finally, this `match_percentage` is printed to the console, indicating how well the resume aligns with the job description based on the textual content and skill relevance.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1da14da"
      },
      "source": [
        "## Resume Analysis Report\n",
        "\n",
        "### Subtask:\n",
        "Summarize the final output, including the overall match percentage, the skills identified in both documents, and the skills missing from the resume based on the job description requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04a41dc9"
      },
      "source": [
        "## Resume Analysis Report\n",
        "\n",
        "This report culminates the resume analysis process, providing a concise overview of the candidate's suitability for the specified job description. It helps in quickly assessing the relevance of a resume by quantifying its alignment with the job requirements.\n",
        "\n",
        "### Report Components:\n",
        "*   **Match Score**: This percentage represents the overall textual and skill alignment between the resume and the job description. A higher score indicates a stronger match in terms of content and keywords.\n",
        "*   **Matched Skills**: This list highlights the specific skills that were explicitly identified in both the candidate's resume and the job description. These are the direct qualifications the candidate possesses that align with the job's needs.\n",
        "*   **Missing Skills**: This list enumerates the skills required by the job description that were not found in the candidate's resume. These indicate potential gaps in the candidate's qualifications or areas where the resume could be further tailored to highlight relevant experience.\n",
        "\n",
        "This report is a valuable tool for both job seekers, who can use it to refine their resumes, and recruiters, who can efficiently screen candidates based on key requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "176fe99d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Conclude the project notes with a summary of the analysis and potential future improvements or next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5b88b69"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The project successfully established a framework for analyzing a resume against a job description, providing an introduction to its purpose of identifying relevant skills and calculating a match percentage.\n",
        "*   Resume processing involved uploading files using `google.colab.files.upload()` and extracting text with a custom `extract_text` function capable of handling both PDF (using `PyPDF2`) and DOCX (using `docx`) formats.\n",
        "*   The job description was defined as a crucial multi-line string input, serving as the benchmark for skill matching and suitability assessment.\n",
        "*   Text preprocessing was thoroughly applied to both the resume and job description via a `clean_text` function, which performed lowercasing, punctuation removal, tokenization, stop word removal, and lemmatization to standardize the text.\n",
        "*   Skill extraction utilized a predefined `skill_list` and an `extract_skills` function to identify unique skills present in both the cleaned resume (`resume_skills`) and the cleaned job description (`job_skills`).\n",
        "*   Similarity between the resume and job description was calculated using a two-step process:\n",
        "    *   **TF-IDF Vectorization** transformed cleaned texts into numerical vectors, weighting words by frequency and rarity.\n",
        "    *   **Cosine Similarity** measured the angular similarity between these vectors, producing a `match_percentage`.\n",
        "*   A comprehensive \"Resume Analysis Report\" was outlined to summarize the final output, detailing the overall match score, identified matched skills, and skills missing from the resume, serving as a valuable tool for both job seekers and recruiters.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The established methodology offers a systematic and quantifiable approach to resume screening, enhancing efficiency for recruiters and providing actionable feedback for job seekers.\n",
        "*   Future enhancements could include implementing more advanced NLP techniques for implicit skill detection, dynamically generating or expanding the `skill_list` from broader industry data, and integrating this analysis into a user-friendly web interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1f198d1"
      },
      "source": [
        "# Task\n",
        "Describe the `app.py` file, explaining its role as the Streamlit frontend for the resume analysis project, and summarize its key functionalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf8d86c"
      },
      "source": [
        "## Describe Streamlit Frontend\n",
        "\n",
        "### Subtask:\n",
        "Explain that `app.py` contains the Streamlit application, which serves as the frontend for the resume analysis project, and summarize its key functionalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04028a57"
      },
      "source": [
        "### Describe Streamlit Frontend\n",
        "\n",
        "`app.py` contains the Python code for a Streamlit web application, serving as the interactive graphical user interface (GUI) or frontend for the entire resume analysis project. This application provides a user-friendly way to interact with the backend analysis logic.\n",
        "\n",
        "#### User Interaction\n",
        "Users can easily engage with the application through its intuitive interface:\n",
        "1.  **Resume Upload**: A `st.file_uploader` widget allows users to upload their resume files, supporting both PDF (`.pdf`) and DOCX (`.docx`) formats.\n",
        "2.  **Job Description Input**: A `st.text_area` provides a convenient space for users to paste the job description against which their resume will be evaluated.\n",
        "3.  **Analysis Trigger**: A `st.button` labeled \"Analyze Resume\" initiates the analysis process once both the resume and job description are provided.\n",
        "\n",
        "#### Core Functionalities Implemented in `app.py`\n",
        "The Streamlit application integrates all the core logic developed in the previous steps to deliver a comprehensive resume analysis report:\n",
        "\n",
        "*   **Text Extraction**: The `extract_text` function (utilizing `PyPDF2` for PDFs and `python-docx` for DOCX files) reads and extracts raw text content from the uploaded resume.\n",
        "*   **Text Cleaning and Preprocessing**: The `clean_text` function performs essential preprocessing steps on both the extracted resume text and the provided job description. This includes lowercasing, removing punctuation, tokenization, filtering out common stop words, and lemmatizing words to their base forms, ensuring a standardized and clean input for analysis.\n",
        "*   **Skill Extraction**: The `extract_skills` function identifies and lists relevant technical skills from both the preprocessed resume and job description by comparing them against a predefined list of industry-standard skills.\n",
        "*   **Similarity Calculation**: The application employs TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to convert the cleaned texts into numerical representations, followed by cosine similarity to compute a quantitative `match_percentage` between the resume and job description.\n",
        "*   **Report Generation**: Finally, the application displays a clear and concise report directly on the web interface, presenting the calculated match score, a list of `matched_skills` (skills found in both documents), and `missing_skills` (skills required by the job but absent from the resume)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "475eb3c6"
      },
      "source": [
        "### Describe Streamlit Frontend\n",
        "\n",
        "`app.py` contains the Python code for a Streamlit web application, serving as the interactive graphical user interface (GUI) or frontend for the entire resume analysis project. This application provides a user-friendly way to interact with the backend analysis logic.\n",
        "\n",
        "#### User Interaction\n",
        "Users can easily engage with the application through its intuitive interface:\n",
        "1.  **Resume Upload**: A `st.file_uploader` widget allows users to upload their resume files, supporting both PDF (`.pdf`) and DOCX (`.docx`) formats.\n",
        "2.  **Job Description Input**: A `st.text_area` provides a convenient space for users to paste the job description against which their resume will be evaluated.\n",
        "3.  **Analysis Trigger**: A `st.button` labeled \"Analyze Resume\" initiates the analysis process once both the resume and job description are provided.\n",
        "\n",
        "#### Core Functionalities Implemented in `app.py`\n",
        "The Streamlit application integrates all the core logic developed in the previous steps to deliver a comprehensive resume analysis report:\n",
        "\n",
        "*   **Text Extraction**: The `extract_text` function (utilizing `PyPDF2` for PDFs and `python-docx` for DOCX files) reads and extracts raw text content from the uploaded resume.\n",
        "*   **Text Cleaning and Preprocessing**: The `clean_text` function performs essential preprocessing steps on both the extracted resume text and the provided job description. This includes lowercasing, removing punctuation, tokenization, filtering out common stop words, and lemmatizing words to their base forms, ensuring a standardized and clean input for analysis.\n",
        "*   **Skill Extraction**: The `extract_skills` function identifies and lists relevant technical skills from both the preprocessed resume and job description by comparing them against a predefined list of industry-standard skills.\n",
        "*   **Similarity Calculation**: The application employs TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to convert the cleaned texts into numerical representations, followed by cosine similarity to compute a quantitative `match_percentage` between the resume and job description.\n",
        "*   **Report Generation**: Finally, the application displays a clear and concise report directly on the web interface, presenting the calculated match score, a list of `matched_skills` (skills found in both documents), and `missing_skills` (skills required by the job but absent from the resume)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28771148"
      },
      "source": [
        "### Describe Streamlit Frontend\n",
        "\n",
        "`app.py` contains the Python code for a Streamlit web application, serving as the interactive graphical user interface (GUI) or frontend for the entire resume analysis project. This application provides a user-friendly way to interact with the backend analysis logic.\n",
        "\n",
        "#### User Interaction\n",
        "Users can easily engage with the application through its intuitive interface:\n",
        "1.  **Resume Upload**: A `st.file_uploader` widget allows users to upload their resume files, supporting both PDF (`.pdf`) and DOCX (`.docx`) formats.\n",
        "2.  **Job Description Input**: A `st.text_area` provides a convenient space for users to paste the job description against which their resume will be evaluated.\n",
        "3.  **Analysis Trigger**: A `st.button` labeled \"Analyze Resume\" initiates the analysis process once both the resume and job description are provided.\n",
        "\n",
        "#### Core Functionalities Implemented in `app.py`\n",
        "The Streamlit application integrates all the core logic developed in the previous steps to deliver a comprehensive resume analysis report:\n",
        "\n",
        "*   **Text Extraction**: The `extract_text` function (utilizing `PyPDF2` for PDFs and `python-docx` for DOCX files) reads and extracts raw text content from the uploaded resume.\n",
        "*   **Text Cleaning and Preprocessing**: The `clean_text` function performs essential preprocessing steps on both the extracted resume text and the provided job description. This includes lowercasing, removing punctuation, tokenization, filtering out common stop words, and lemmatizing words to their base forms, ensuring a standardized and clean input for analysis.\n",
        "*   **Skill Extraction**: The `extract_skills` function identifies and lists relevant technical skills from both the preprocessed resume and job description by comparing them against a predefined list of industry-standard skills.\n",
        "*   **Similarity Calculation**: The application employs TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to convert the cleaned texts into numerical representations, followed by cosine similarity to compute a quantitative `match_percentage` between the resume and job description.\n",
        "*   **Report Generation**: Finally, the application displays a clear and concise report directly on the web interface, presenting the calculated match score, a list of `matched_skills` (skills found in both documents), and `missing_skills` (skills required by the job but absent from the resume).\n"
      ]
    }
  ]
}